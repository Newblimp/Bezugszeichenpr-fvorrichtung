#!/usr/bin/env python3
"""
Embed ONNX model files as uncompressed hex arrays in C++

Converts binary model files to C++ source code with embedded hex data.
Each model becomes a const uint8_t array with accompanying size constant.
"""

import argparse
import os
import sys


def bytes_to_hex_array(data, bytes_per_line=16):
    """Convert binary data to formatted hex array lines."""
    lines = []
    for i in range(0, len(data), bytes_per_line):
        chunk = data[i:i + bytes_per_line]
        hex_bytes = ', '.join(f'0x{b:02x}' for b in chunk)
        lines.append(f'    {hex_bytes},')
    return lines


def generate_cpp_file(det_path, encoder_path, decoder_path, vocab_path, output_path):
    """Generate C++ source file with embedded models."""

    # Read model files
    print(f"Reading detection model: {det_path}")
    with open(det_path, 'rb') as f:
        det_data = f.read()

    print(f"Reading TrOCR encoder model: {encoder_path}")
    with open(encoder_path, 'rb') as f:
        encoder_data = f.read()

    print(f"Reading TrOCR decoder model: {decoder_path}")
    with open(decoder_path, 'rb') as f:
        decoder_data = f.read()

    print(f"Reading vocabulary: {vocab_path}")
    with open(vocab_path, 'rb') as f:
        vocab_data = f.read()

    print(f"Detection model size: {len(det_data)} bytes")
    print(f"TrOCR encoder size: {len(encoder_data)} bytes")
    print(f"TrOCR decoder size: {len(decoder_data)} bytes")
    print(f"Vocabulary size: {len(vocab_data)} bytes")

    # Convert to hex arrays
    det_hex_lines = bytes_to_hex_array(det_data)
    encoder_hex_lines = bytes_to_hex_array(encoder_data)
    decoder_hex_lines = bytes_to_hex_array(decoder_data)
    vocab_hex_lines = bytes_to_hex_array(vocab_data)

    # Generate C++ code
    cpp_code = """#ifdef HAVE_OPENCV

#include <cstdint>
#include <cstddef>

// Auto-generated: Do not edit manually
// Embedded OCR models: Detection (YOLOv11n) + Recognition (TrOCR)
// Generated by embed_models.py

"""

    # Detection model
    cpp_code += f"""// Detection model (YOLOv11n)
// Size: {len(det_data)} bytes
extern const uint8_t det_model_uncompressed[] = {{
"""
    cpp_code += '\n'.join(det_hex_lines)
    cpp_code += f"""
}};
extern const size_t det_model_uncompressed_size = {len(det_data)};

"""

    # TrOCR Encoder
    cpp_code += f"""// TrOCR Encoder model (Vision Transformer)
// Size: {len(encoder_data)} bytes
extern const uint8_t trocr_encoder_uncompressed[] = {{
"""
    cpp_code += '\n'.join(encoder_hex_lines)
    cpp_code += f"""
}};
extern const size_t trocr_encoder_uncompressed_size = {len(encoder_data)};

"""

    # TrOCR Decoder
    cpp_code += f"""// TrOCR Decoder model (GPT-2 style autoregressive)
// Size: {len(decoder_data)} bytes
extern const uint8_t trocr_decoder_uncompressed[] = {{
"""
    cpp_code += '\n'.join(decoder_hex_lines)
    cpp_code += f"""
}};
extern const size_t trocr_decoder_uncompressed_size = {len(decoder_data)};

"""

    # Vocabulary
    cpp_code += f"""// TrOCR vocabulary (one token per line, 64002 tokens)
// Size: {len(vocab_data)} bytes
extern const uint8_t trocr_vocab_uncompressed[] = {{
"""
    cpp_code += '\n'.join(vocab_hex_lines)
    cpp_code += f"""
}};
extern const size_t trocr_vocab_uncompressed_size = {len(vocab_data)};

#endif // HAVE_OPENCV
"""

    # Write output file
    print(f"Writing output: {output_path}")
    with open(output_path, 'w') as f:
        f.write(cpp_code)

    print("Model embedding completed successfully")


def main():
    parser = argparse.ArgumentParser(
        description='Embed ONNX models as uncompressed hex arrays in C++')
    parser.add_argument('--det', required=True, help='Path to detection model')
    parser.add_argument('--encoder', required=True, help='Path to TrOCR encoder model')
    parser.add_argument('--decoder', required=True, help='Path to TrOCR decoder model')
    parser.add_argument('--vocab', required=True, help='Path to vocabulary file')
    parser.add_argument('--output', required=True, help='Output C++ file path')

    args = parser.parse_args()

    # Validate input files
    for name, path in [('Detection', args.det), ('Encoder', args.encoder),
                       ('Decoder', args.decoder), ('Vocabulary', args.vocab)]:
        if not os.path.exists(path):
            print(f"Error: {name} file not found: {path}", file=sys.stderr)
            sys.exit(1)

    # Create output directory if needed
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)

    try:
        generate_cpp_file(args.det, args.encoder, args.decoder, args.vocab, args.output)
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == '__main__':
    main()
